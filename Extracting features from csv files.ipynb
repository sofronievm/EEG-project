{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "# import scipy.signal as sig\n",
    "import sys\n",
    "from pylab import *\n",
    "# import scipy.signal\n",
    "from spectrum import *\n",
    "import time\n",
    "from IPython.display import clear_output\n",
    "import pycwt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "# path = 'G:/EEG Project/Hut lab data/30_sec_epochs/data_raw/'\n",
    "# start2 = time.time()\n",
    "# name_feature = 'cwt_spectral'\n",
    "# channels = ['Oz-Cz', 'Oz-Fpz', 'Oz-C3', 'Oz-C4', 'Cz-Fpz', 'Cz-C3', 'Cz-C4', 'Fpz-C3', 'Fpz-C4', 'C3-C4'] #\n",
    "# print('\\n Starting feature extraction of .csv files...')\n",
    "\n",
    "# for channel in channels:\n",
    "#     start = time.time()\n",
    "#     print('\\n channel: ', channel)\n",
    "#     csv_files = glob.glob(path + '*'+channel+'.csv')\n",
    "#     score_files = glob.glob(path + '*txt.csv')\n",
    "#     for csv, txt in zip(csv_files, score_files):\n",
    "        \n",
    "#         cohl_feature = []\n",
    "#         df = pd.read_csv(csv, index_col=0)\n",
    "#         df.fillna(0, inplace=True)\n",
    "#         tf = pd.read_csv(txt, index_col=0)['Score']\n",
    "#         size = int(df.size/len(df))\n",
    "        \n",
    "#         print('\\n', csv, size)\n",
    "#         print(txt, len(txt))\n",
    "        \n",
    "#         for i in range(size):\n",
    "#             np.seterr(all='ignore')\n",
    "#             epoch = np.asarray(df.iloc[:, i])\n",
    "#             epoch = epoch[~np.isnan(epoch)]\n",
    "            \n",
    "#             fs = 128\n",
    "#             dt = 1/fs\n",
    "#             wave, scales, freqs, coi, fft, fftfreqs = pycwt.wavelet.cwt(epoch, dt)   #, dj, s0, J, mother\n",
    "#             cohl = np.abs(wave)\n",
    "#             cohl = cohl[::-1]\n",
    "#             cohl_flat= cohl.flatten()\n",
    "#             splits = [318720, 360960, 395520, 445440]\n",
    "#             cwt_spectral = np.split(cohl_flat, splits)\n",
    "            \n",
    "#             cohl_m = []\n",
    "#             cohl_mcr = []\n",
    "#             cohl_p = []\n",
    "#             cohl_c = []\n",
    "#             cohl_f = []\n",
    "#             cohl_v = []\n",
    "#             cohl_r = []\n",
    "\n",
    "#             for s in range(5):\n",
    "#                 cohl_m.append(np.mean(cwt_spectral[s]))\n",
    "#                 cohl_mcr.append((np.nonzero(np.diff(cwt_spectral[s] > np.mean(cwt_spectral[s])))[0]).size)\n",
    "#                 cohl_p.append(np.trapz(cwt_spectral[s]))\n",
    "#                 cohl_c.append(np.sum(cwt_spectral[s]*len(cwt_spectral[s]))/np.sum(cwt_spectral[s]))\n",
    "#                 cohl_f.append(scp.stats.mstats.gmean(cwt_spectral[s])/np.mean(cwt_spectral[s]))\n",
    "#                 cohl_v.append(np.var(cwt_spectral[s]))\n",
    "#                 cohl_r.append(0.95*np.sum(cwt_spectral[s]))\n",
    "                \n",
    "#             cohl_feature.append([cohl_m[0], cohl_mcr[0], cohl_p[0], cohl_c[0], cohl_f[0], cohl_v[0], cohl_r[0],\n",
    "#                                 cohl_m[1], cohl_mcr[1], cohl_p[1], cohl_c[1], cohl_f[1], cohl_v[1], cohl_r[1], \n",
    "#                                 cohl_m[2], cohl_mcr[2], cohl_p[2], cohl_c[2], cohl_f[2], cohl_v[2], cohl_r[2],\n",
    "#                                 cohl_m[3], cohl_mcr[3], cohl_p[3], cohl_c[3], cohl_f[3], cohl_v[3], cohl_r[3], \n",
    "#                                 cohl_m[4], cohl_mcr[4], cohl_p[4], cohl_c[4], cohl_f[4], cohl_v[4], cohl_r[4]])\n",
    "#             sys.stdout.write(\"\\r%d%% epochs processed\" % i)\n",
    "                                 \n",
    "#         new_df = pd.DataFrame(cohl_feature)                               \n",
    "#         new_df.columns = ['Dm', 'Dmcr', 'Dp', 'Dc', 'Df' ,'Dv', 'Dr', \n",
    "#                          'Tm', 'Tmcr', 'Tp', 'Tc', 'Tf' ,'Tv', 'Tr',\n",
    "#                          'Am', 'Amcr', 'Ap', 'Ac', 'Af' ,'Av', 'Ar', \n",
    "#                          'Bm', 'Bmcr', 'Bp', 'Bc', 'Bf' ,'Bv', 'Br',\n",
    "#                          'Gm', 'Gmcr', 'Gp', 'Gc', 'Gf' ,'Gv', 'Gr']\n",
    "                                 \n",
    "#         big_df = pd.concat([new_df, tf], axis=1)\n",
    "#         big_df = big_df[~np.isnan(big_df['Score'])]\n",
    "#         big_df = big_df[big_df.Score < 7]\n",
    "#         big_df = big_df[~np.isnan(big_df['Gc'])]\n",
    "#         print('\\n Writing feature set to .csv file...')\n",
    "#         toPrint = csv.split('raw\\\\', 1)[1]\n",
    "#         toPrint = toPrint.split('.edf', 1)[0]\n",
    "#         big_df.to_csv('G:/EEG Project/Hut lab data/30_sec_epochs/cwt_spectral features/'+channel+'/'+toPrint+'_'+name_feature+'.csv')\n",
    "   \n",
    "#     end = time.time()\n",
    "#     elapsed = (end - start)/60\n",
    "#     print(\"Time to complete: %.2f\" % elapsed, \"mins\")\n",
    "                                 \n",
    "# end2 = time.time()\n",
    "# elapsed2 = (end2-start2)/60\n",
    "# print('time for the whole dataset: ', elapsed2, ' mins')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n",
      "done...\n"
     ]
    }
   ],
   "source": [
    "channels = ['Oz-Cz', 'Oz-Fpz', 'Oz-C3', 'Oz-C4', 'Cz-Fpz', 'Cz-C3', 'Cz-C4', 'Fpz-C3', 'Fpz-C4', 'C3-C4'] \n",
    "for channel in channels:\n",
    "    path = 'G:/EEG Project/Hut lab data/10_sec_epochs/cwt_spectral features/'+channel+'/'\n",
    "    output_file = []\n",
    "\n",
    "    for filename in glob.glob(os.path.join(path, '*.csv')):\n",
    "        df = pd.read_csv(filename, index_col=0)\n",
    "        output_file.append(df)\n",
    "    new_frame = pd.concat(output_file, axis=0, ignore_index=True)\n",
    "    new_frame.to_csv('G:/EEG Project/Hut lab data/10_sec_epochs/cwt_spectral features/'+channel+'/output.csv')\n",
    "    print('done...')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # df = pd.read_csv('G:/EEG Project/time_features_Oz-Fpz/combined_features.csv', index_col=0)['scores']\n",
    "# df2 = pd.read_csv('G:/EEG Project/features/AR_coeff.csv', index_col=0)\n",
    "# df3 = pd.concat([df2, df], axis=1)\n",
    "# df3.to_csv('G:/EEG Project/features/Parametric.csv')\n",
    "# df3.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.read_csv('features/Bands_f_c.csv',index_col=0)\n",
    "# features = features[~np.isnan(features['scores'])]\n",
    "# features = features[features['scores'] < 7]\n",
    "# data = features[['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']]\n",
    "# labels = features['scores']\n",
    "# from sklearn.decomposition import PCA\n",
    "\n",
    "# pca = PCA(n_components=2)\n",
    "# pca.fit(data)\n",
    "# components = pca.transform(data)\n",
    "# pca_c = pd.DataFrame(components)\n",
    "# pca_c.columns = ['PCA1', 'PCA2']\n",
    "# pca_res = pd.concat([pca_c, labels], axis=1)\n",
    "# pca_res.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# import seaborn as sns\n",
    "\n",
    "# features = pd.read_csv('G:/EEG Project/Hut lab data/10_sec_epochs/C3-C4 features/output.csv',index_col=0)\n",
    "# # features = features[~np.isnan(features['scores'])]\n",
    "# # features = features[features['scores'] < 10]\n",
    "# # features.iloc[:, 27:].head()\n",
    "# %matplotlib inline\n",
    "# # flatui = [\"#9b59b6\", \"#3498db\", \"#95a5a6\", \"#e74c3c\", \"#34495e\", \"#2ecc71\", ]\n",
    "# # palette = sns.color_palette(flatui)\n",
    "# import seaborn as sns; sns.set()\n",
    "\n",
    "# sns_pair = sns.pairplot(features, hue='scores', size=1.5, palette=\"dark\")\n",
    "# sns_pair.savefig('G:/EEG Project/Hut lab data/10_sec_epochs/C3-C4 features/tf_7_scores.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# features = pd.read_csv('G:/EEG Project/DREAMS database/STFT features/ch5-ch6/output.csv', index_col=0)\n",
    "# features['scores'].replace([6, 5], 7, inplace=True)\n",
    "# features['scores'].replace([0, 1, 2, 3], 8, inplace=True)\n",
    "# features['scores'].replace(4, 9, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import os\n",
    "import glob\n",
    "import math\n",
    "import pandas as pd\n",
    "import scipy as scp\n",
    "from scipy import signal\n",
    "import sys\n",
    "from pylab import *\n",
    "# import scipy.signal\n",
    "from spectrum import *\n",
    "import time\n",
    "from sklearn import svm\n",
    "from sklearn.metrics import confusion_matrix, accuracy_score\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.naive_bayes import GaussianNB\n",
    "from sklearn.cross_validation import train_test_split\n",
    "from sklearn.model_selection import cross_val_score\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.ensemble import AdaBoostClassifier\n",
    "from sklearn.datasets import make_hastie_10_2\n",
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "import time\n",
    "\n",
    "features = pd.read_csv('G:/EEG Project/Hut lab data/30_sec_epochs/cohl_fb features/Oz-Cz/output.csv',index_col=0)\n",
    "# features = features[features.Score != 6]\n",
    "# features['Score'].replace([3, 4], 3, inplace=True)\n",
    "# # features['Score'].replace([1, 2, 3, 4], 8, inplace=True)\n",
    "# features['Score'].replace(5, 4, inplace=True)\n",
    "\n",
    "# ''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''''\n",
    "# features['Score'].replace([3, 4], 8, inplace=True)\n",
    "# features = features[features.Score < 11]\n",
    "# features = features[~np.isnan(features['scores'])]\n",
    "# features = features[~np.isnan(features['AR_7'])]\n",
    "# features = features[features.Score < 10]\n",
    "X = features.loc[:, features.columns != 'Score']\n",
    "# X = features[['ZCR', 'H_complexity', 'std', 'min', 'max']]\n",
    "y = features['Score']\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size = 0.21, random_state=0)\n",
    "# start = time.time()\n",
    "# clf = KNeighborsClassifier(n_neighbors=30)\n",
    "clf = RandomForestClassifier()\n",
    "clf.fit(X_train, y_train)\n",
    "# clf = AdaBoostClassifier(n_estimators=100).fit(X_train, y_train)\n",
    "# scores = cross_val_score(clf, X_train, y_train)\n",
    "# print(scores.mean())\n",
    "# clf = GradientBoostingClassifier(n_estimators=100, learning_rate=1.0, max_depth=1, random_state=0).fit(X_train, y_train)\n",
    "# end = time.time()\n",
    "# elapsed = end - start\n",
    "#     print(\"Time to complete: \", elapsed, \"seconds\")\n",
    "\n",
    "figure(1)\n",
    "importances = clf.feature_importances_\n",
    "std = np.std([tree.feature_importances_ for tree in clf.estimators_],\n",
    "             axis=0)\n",
    "indices = np.argsort(importances)[::-1]\n",
    "features_names = X_train.columns # e.g. ['A', 'B', 'C', 'D', 'E']\n",
    "features_names = [features_names[i] for i in indices]\n",
    "\n",
    "# feature_names = [feature_names[i] for i in indices]\n",
    "\n",
    "figure(1)\n",
    "plt.figure(figsize = (15,8))\n",
    "plt.title(\"Feature importances\")\n",
    "plt.bar(range(X_train.shape[1]), importances[indices],\n",
    "       color=\"r\", yerr=std[indices], align=\"center\")\n",
    "plt.xticks(range(X_train.shape[1]), features_names, rotation=90)\n",
    "plt.xlim([-1, X_train.shape[1]])\n",
    "# plt.show()\n",
    "\n",
    "predictions = clf.predict(X_test)\n",
    "# print(filename)\n",
    "accuracy = accuracy_score(y_test, predictions)\n",
    "print(\"Accuracy: %.1f\" % (accuracy*100))\n",
    "CM = confusion_matrix(y_test, predictions)\n",
    "FP = CM.sum(axis=0) - np.diag(CM)  \n",
    "FN = CM.sum(axis=1) - np.diag(CM)\n",
    "TP = np.diag(CM)\n",
    "TN = CM.sum() - (FP + FN + TP)\n",
    "Sensitivity = TP/(TP+FN)\n",
    "Specificity = TN/(TN+FP) \n",
    "np.set_printoptions(precision=1)\n",
    "print(\"Sensitivity: \", np.multiply(Sensitivity, 100))\n",
    "print(\"Specificity: \", np.multiply(Specificity, 100))\n",
    "# conmat = confusion_matrix(y_test, predictions)\n",
    "# df_cm = pd.DataFrame(conmat, index = ['NREM','REM','N/S'],\n",
    "#                   columns = ['NREM','REM','N/S'])\n",
    "# figure(2)\n",
    "# plt.figure(figsize = (15,8))\n",
    "# sns.heatmap(df_cm, annot=True,annot_kws={\"size\": 16})# font size"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# path = 'features/'\n",
    "# data = []\n",
    "# for idx, filename in enumerate(glob.glob(os.path.join(path, '*.csv'))):\n",
    "#     df = pd.read_csv(filename, index_col=0)\n",
    "#     data.append(df)\n",
    "# # print(data)\n",
    "# bigframe = pd.concat(data, axis=1) #314dont want pandas to try an align row indexes\n",
    "# bigframe.to_csv(\"features/combined_features.csv\")\n",
    "# bigframe.head()    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Some useful features\n",
    "### Time-Domain\n",
    "        mean = np.mean(epoch)\n",
    "        std = np.std(epoch)\n",
    "        tw_five = np.percentile(epoch,  25)  \n",
    "        sv_five = np.percentile(epoch,  75)  \n",
    "        kurtosis = scp.stats.kurtosis(epoch)\n",
    "        skewness = scp.stats.skew(epoch)\n",
    "        minimum = np.amin(epoch)\n",
    "        maximum = np.amax(epoch)\n",
    "        ZCR = (np.nonzero(np.diff(epoch > 0))[0]).size\n",
    "        first_deriv = np.diff(epoch)\n",
    "        second_deriv = np.diff(epoch,2)\n",
    "\n",
    "        var_zero = np.mean(epoch ** 2)\n",
    "        var_d1 = np.mean(first_deriv ** 2)\n",
    "        var_d2 = np.mean(second_deriv ** 2)\n",
    "\n",
    "        activity = var_zero\n",
    "        morbidity = np.sqrt(var_d1 / var_zero)\n",
    "        complexity = np.sqrt(var_d2 / var_d1) / morbidity        \n",
    "        features.append([mean, std, tw_five, sv_five, kurtosis, skewness, minimum, maximum, ZCR, activity, morbidity, complexity])\n",
    "        new_df.columns = ['mean', 'std', '25%', '75%', 'kurtosis', 'skewness', 'min', 'max', 'ZCR', 'H_activity', 'H_mobility', 'H_complexity']\n",
    "\n",
    "### Frequency-domain\n",
    "#### PSD of bands (Non-paramteric freq analisys)\n",
    "        fs = 200\n",
    "        fft_vals = np.absolute(np.fft.rfft(epoch)**2)\n",
    "        ''' If dB is wanted {not reccomended in this case}\n",
    "        fft_vals_dB = 10*np.log10(fft_vals)\n",
    "        '''\n",
    "        fft_freq = np.fft.rfftfreq(int(len(epoch)), 1.0/fs)\n",
    "        \n",
    "        eeg_bands = {'Delta': (0.1, 4.3),\n",
    "             'Theta': (3.7, 8.3),\n",
    "             'Alpha': (7.7, 14.3),\n",
    "             'Beta': (13.7, 30.3),\n",
    "             'Gamma': (29.7, 45.3)}\n",
    "        eeg_band_fft = dict()\n",
    "        \n",
    "        for band in eeg_bands:  \n",
    "            freq_ix = np.where((fft_freq >= eeg_bands[band][0]) & \n",
    "                               (fft_freq <= eeg_bands[band][1]))[0]\n",
    "            eeg_band_fft[band] = np.trapz(fft_vals[freq_ix])\n",
    "        bands_int.append([eeg_band_fft[band] for band in eeg_bands])\n",
    "        \n",
    "#### AR process coeffs (Parametric freq analysis)\n",
    "\n",
    "         order_AR = 30\n",
    "         AR, var, k = aryule(epoch, order_AR)\n",
    "         AR_coeff.append(AR)\n",
    "         \n",
    "### STFT first (everything)\n",
    "        fs = 128\n",
    "        window_size = 64\n",
    "        overlap = 0.75*window_size\n",
    "        f, t, Zxx = signal.stft(epoch, fs, window='hamming', nperseg=window_size, detrend=False, boundary='zeros')\n",
    "        stft_x = np.abs(Zxx)**2\n",
    "        full_data = []\n",
    "        for j, freq in enumerate(f):\n",
    "            for k, time in enumerate(t):\n",
    "                full_data.append([freq, time, stft_x[j][k]])\n",
    "\n",
    "        new_frame = pd.DataFrame(full_data)\n",
    "        new_frame.columns=['freq', 'time', 'stft']\n",
    "        df_higher = new_frame.sort_values(by=['stft'],ascending=False)\n",
    "        df_selected = df_higher.head(15)\n",
    "        df_flat = df_selected.values.flatten().transpose()\n",
    "        STFT.append(df_flat)\n",
    "        \n",
    "### STFT second (only singular positions)\n",
    "        fs = 128\n",
    "        window_size = 64\n",
    "        overlap = 0.75*window_size\n",
    "        length_pos = 15\n",
    "        f, t, Zxx = signal.stft(epoch, fs, window='hamming', nperseg=window_size, detrend=False, boundary='zeros')\n",
    "        stft_x = np.abs(Zxx)**2\n",
    "        stft_flat = stft_x.flatten()\n",
    "######        pos = stft_flat.argsort()[-15:][::-1]\n",
    "        splits = [65, 131, 230, 494]\n",
    "        stft_bands = np.split(stft_flat, splits)\n",
    "        stft_fb = []\n",
    "        for s in range(5):\n",
    "            stft_fb.append(np.mean(stft_bands[s]))\n",
    "        STFT.append(stft_fb)\n",
    "        \n",
    "### Proper PSD\n",
    "        sys.stdout.flush()\n",
    "        np.seterr(all='ignore')\n",
    "        epoch = np.asarray(df.iloc[:, i])\n",
    "        epoch = epoch[~np.isnan(epoch)]\n",
    "        fs = 128\n",
    "        N = 128  # length of segments\n",
    "        M = 64   # stepsize\n",
    "        fft_freq, fft_vals = sig.welch(epoch, fs, window='hamming', nperseg=N, noverlap=(N-M))\n",
    "        ''' If dB is wanted {not reccomended in this case}\n",
    "        fft_vals_dB = 10*np.log10(fft_vals)\n",
    "        '''\n",
    "        eeg_bands = {'Delta': (0.1, 4.3),\n",
    "             'Theta': (3.7, 8.3),\n",
    "             'Alpha': (7.7, 14.3),\n",
    "             'Beta': (13.7, 30.3),\n",
    "             'Gamma': (29.7, 45.3)}\n",
    "        eeg_band_fft = dict()\n",
    "        \n",
    "        for band in eeg_bands:  \n",
    "            freq_ix = np.where((fft_freq >= eeg_bands[band][0]) & \n",
    "                               (fft_freq <= eeg_bands[band][1]))[0]\n",
    "            eeg_band_fft[band] = np.trapz(fft_vals[freq_ix])\n",
    "        psd_band_int.append([eeg_band_fft[band] for band in eeg_bands])\n",
    "        sys.stdout.write(\"\\r%d%% epochs processed\" % i)\n",
    "    new_df = pd.DataFrame(psd_band_int)\n",
    "    new_df.columns = ['Delta', 'Theta', 'Alpha', 'Beta', 'Gamma']\n",
    "    tf.columns = ['scores']\n",
    "    big_df = pd.concat([new_df, tf], axis=1)\n",
    "    big_df = big_df[~np.isnan(big_df['scores'])]\n",
    "    big_df = big_df[big_df.scores < 7]\n",
    "    print('\\n Writing feature set to .csv file...')\n",
    "    toPrint = csv.split('raw\\\\', 1)[1]\n",
    "    toPrint = toPrint.split('.edf', 1)[0]\n",
    "    big_df.to_csv('G:/EEG Project/Hut lab data/10_sec_epochs/psd_band_int/Oz-C4/'+toPrint+'_'+name_feature+'.csv')\n",
    "    \n",
    "### something from the wavelets\n",
    "    dt = 1/128\n",
    "    mother = pycwt.wavelet.Morlet(6.)\n",
    "    wave, scales, freqs, coi, fft, fftfreqs = pycwt.wavelet.cwt(epoch, dt)\n",
    "    cohl = abs(wave)\n",
    "    nrows = 22\n",
    "    ncols = 640\n",
    "    n = 36\n",
    "    blocks = np.array((n, nrows, ncols))\n",
    "    h, w = cohl.shape\n",
    "    blocks = (cohl.reshape(h//nrows, nrows, -1, ncols).swapaxes(1,2).reshape(-1, nrows, ncols))\n",
    "    spec_coeffs = np.zeros((36, 1)) \n",
    "    for i in range(n):\n",
    "        spec_coeffs[i] = np.linalg.norm(blocks[i]).flatten().transpose()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_features(channel_name):\n",
    "    path = 'G:/EEG Project/Hut lab data/30_sec_epochs/data_raw/'\n",
    "    channel= channel_name\n",
    "    start = time.time()\n",
    "    print('\\n channel: ', channel)\n",
    "    csv_files = glob.glob(path + '*'+channel+'.csv')\n",
    "    score_files = glob.glob(path + '*txt.csv')\n",
    "    \n",
    "    cwt_spectral(csv_files, score_files, channel)\n",
    "    \n",
    "    end = time.time()\n",
    "    elapsed = (end - start)/60\n",
    "    print(\"Time to complete: %.2f\" % elapsed, \"mins\")\n",
    "    \n",
    "    \n",
    "def cwt_spectral(csv_files, score_files, channel):\n",
    "        for csv, txt in zip(csv_files, score_files):\n",
    "        name_feature = 'cwt_spectral'\n",
    "        cohl_feature = []\n",
    "        df = pd.read_csv(csv, index_col=0)\n",
    "        df.fillna(0, inplace=True)\n",
    "        tf = pd.read_csv(txt, index_col=0)['Score']\n",
    "        size = int(df.size/len(df))\n",
    "\n",
    "        for i in range(size):\n",
    "            np.seterr(all='ignore')\n",
    "            epoch = np.asarray(df.iloc[:, i])\n",
    "            epoch = epoch[~np.isnan(epoch)]\n",
    "\n",
    "            fs = 128\n",
    "            dt = 1/fs\n",
    "            wave, scales, freqs, coi, fft, fftfreqs = pycwt.wavelet.cwt(epoch, dt)   #, dj, s0, J, mother\n",
    "            cohl = np.abs(wave)\n",
    "            cohl = cohl[::-1]\n",
    "            cohl_flat= cohl.flatten()\n",
    "            splits = [318720, 360960, 395520, 445440]\n",
    "            cwt_spectral = np.split(cohl_flat, splits)\n",
    "\n",
    "            cohl_m = []\n",
    "            cohl_mcr = []\n",
    "            cohl_p = []\n",
    "            cohl_c = []\n",
    "            cohl_f = []\n",
    "            cohl_v = []\n",
    "            cohl_r = []\n",
    "\n",
    "            for s in range(5):\n",
    "                cohl_m.append(np.mean(cwt_spectral[s]))\n",
    "                cohl_mcr.append((np.nonzero(np.diff(cwt_spectral[s] > np.mean(cwt_spectral[s])))[0]).size)\n",
    "                cohl_p.append(np.trapz(cwt_spectral[s]))\n",
    "                cohl_c.append(np.sum(cwt_spectral[s]*len(cwt_spectral[s]))/np.sum(cwt_spectral[s]))\n",
    "                cohl_f.append(scp.stats.mstats.gmean(cwt_spectral[s])/np.mean(cwt_spectral[s]))\n",
    "                cohl_v.append(np.var(cwt_spectral[s]))\n",
    "                cohl_r.append(0.95*np.sum(cwt_spectral[s]))\n",
    "\n",
    "            cohl_feature.append([cohl_m[0], cohl_mcr[0], cohl_p[0], cohl_c[0], cohl_f[0], cohl_v[0], cohl_r[0],\n",
    "                                cohl_m[1], cohl_mcr[1], cohl_p[1], cohl_c[1], cohl_f[1], cohl_v[1], cohl_r[1], \n",
    "                                cohl_m[2], cohl_mcr[2], cohl_p[2], cohl_c[2], cohl_f[2], cohl_v[2], cohl_r[2],\n",
    "                                cohl_m[3], cohl_mcr[3], cohl_p[3], cohl_c[3], cohl_f[3], cohl_v[3], cohl_r[3], \n",
    "                                cohl_m[4], cohl_mcr[4], cohl_p[4], cohl_c[4], cohl_f[4], cohl_v[4], cohl_r[4]])\n",
    "            sys.stdout.write(\"\\r%d%% epochs processed\" % i)\n",
    "\n",
    "        new_df = pd.DataFrame(cohl_feature)                               \n",
    "        new_df.columns = ['Dm', 'Dmcr', 'Dp', 'Dc', 'Df' ,'Dv', 'Dr', \n",
    "                         'Tm', 'Tmcr', 'Tp', 'Tc', 'Tf' ,'Tv', 'Tr',\n",
    "                         'Am', 'Amcr', 'Ap', 'Ac', 'Af' ,'Av', 'Ar', \n",
    "                         'Bm', 'Bmcr', 'Bp', 'Bc', 'Bf' ,'Bv', 'Br',\n",
    "                         'Gm', 'Gmcr', 'Gp', 'Gc', 'Gf' ,'Gv', 'Gr']\n",
    "\n",
    "        big_df = pd.concat([new_df, tf], axis=1)\n",
    "        big_df = big_df[~np.isnan(big_df['Score'])]\n",
    "        big_df = big_df[big_df.Score < 7]\n",
    "        big_df = big_df[~np.isnan(big_df['Gr'])]\n",
    "        toPrint = csv.split('raw\\\\', 1)[1]\n",
    "        toPrint = toPrint.split('.edf', 1)[0]\n",
    "        big_df.to_csv('G:/EEG Project/Hut lab data/30_sec_epochs/cwt_spectral features/'+channel+'/'+toPrint+'_'+name_feature+'.csv')\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
